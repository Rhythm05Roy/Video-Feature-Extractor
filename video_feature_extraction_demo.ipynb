{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¨ Video Feature Extraction Tool\n",
                "\n",
                "**All-in-One Notebook** - Complete video analysis solution with all code embedded.\n",
                "\n",
                "## Features:\n",
                "- **Shot Cut Detection**: Counts hard cuts using frame-to-frame pixel analysis\n",
                "- **Motion Analysis**: Computes average motion magnitude via optical flow\n",
                "- **Text Detection (OCR)**: Detects text presence and extracts keywords\n",
                "- **Object/Person Detection**: Estimates person vs object dominance using YOLOv8\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies\n",
                "\n",
                "Run this cell once to install required packages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment and run to install dependencies\n",
                "# !pip install numpy opencv-python pillow pytesseract ultralytics pyyaml tqdm -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Core Implementation\n",
                "\n",
                "All the video feature extraction code is embedded below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Core imports loaded\n",
                        "   OpenCV: 4.12.0\n",
                        "   Tesseract: Available\n",
                        "   YOLO: Available\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# IMPORTS\n",
                "# ============================================================================\n",
                "\n",
                "import json\n",
                "import logging\n",
                "import sys\n",
                "from abc import ABC, abstractmethod\n",
                "from collections import Counter\n",
                "from dataclasses import dataclass, field\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "\n",
                "# Optional imports\n",
                "try:\n",
                "    import pytesseract\n",
                "    TESSERACT_AVAILABLE = True\n",
                "except ImportError:\n",
                "    TESSERACT_AVAILABLE = False\n",
                "    pytesseract = None\n",
                "\n",
                "try:\n",
                "    from ultralytics import YOLO\n",
                "    YOLO_AVAILABLE = True\n",
                "except ImportError:\n",
                "    YOLO_AVAILABLE = False\n",
                "    YOLO = None\n",
                "\n",
                "print(\"‚úÖ Core imports loaded\")\n",
                "print(f\"   OpenCV: {cv2.__version__}\")\n",
                "print(f\"   Tesseract: {'Available' if TESSERACT_AVAILABLE else 'Not installed'}\")\n",
                "print(f\"   YOLO: {'Available' if YOLO_AVAILABLE else 'Not installed'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Exception classes defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# CUSTOM EXCEPTIONS\n",
                "# ============================================================================\n",
                "\n",
                "class VideoFeatureExtractorError(Exception):\n",
                "    \"\"\"Base exception for all Video Feature Extractor errors.\"\"\"\n",
                "    def __init__(self, message: str, details: dict = None):\n",
                "        super().__init__(message)\n",
                "        self.message = message\n",
                "        self.details = details or {}\n",
                "\n",
                "class VideoNotFoundError(VideoFeatureExtractorError):\n",
                "    \"\"\"Raised when the specified video file does not exist.\"\"\"\n",
                "    def __init__(self, video_path: str):\n",
                "        super().__init__(f\"Video file not found: {video_path}\")\n",
                "        self.video_path = video_path\n",
                "\n",
                "class VideoOpenError(VideoFeatureExtractorError):\n",
                "    \"\"\"Raised when a video file cannot be opened.\"\"\"\n",
                "    def __init__(self, video_path: str, reason: str = None):\n",
                "        message = f\"Unable to open video: {video_path}\"\n",
                "        if reason:\n",
                "            message += f\" - {reason}\"\n",
                "        super().__init__(message)\n",
                "        self.video_path = video_path\n",
                "\n",
                "class InvalidFeatureError(VideoFeatureExtractorError):\n",
                "    \"\"\"Raised when an invalid feature name is requested.\"\"\"\n",
                "    def __init__(self, requested: list, valid: set):\n",
                "        invalid = set(requested) - valid\n",
                "        super().__init__(f\"Invalid feature(s): {invalid}. Valid: {valid}\")\n",
                "\n",
                "print(\"‚úÖ Exception classes defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Configuration classes defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass\n",
                "class ShotCutConfig:\n",
                "    \"\"\"Configuration for shot cut detection.\"\"\"\n",
                "    enabled: bool = True\n",
                "    frame_step: int = 1\n",
                "    diff_threshold: float = 30.0\n",
                "    min_gap_frames: int = 5\n",
                "\n",
                "@dataclass\n",
                "class MotionConfig:\n",
                "    \"\"\"Configuration for motion analysis.\"\"\"\n",
                "    enabled: bool = True\n",
                "    frame_step: int = 2\n",
                "    pyr_scale: float = 0.5\n",
                "    levels: int = 3\n",
                "    winsize: int = 15\n",
                "    iterations: int = 3\n",
                "    poly_n: int = 5\n",
                "    poly_sigma: float = 1.2\n",
                "\n",
                "@dataclass\n",
                "class TextDetectionConfig:\n",
                "    \"\"\"Configuration for OCR text detection.\"\"\"\n",
                "    enabled: bool = True\n",
                "    frame_step: int = 15\n",
                "    min_confidence: float = 70.0\n",
                "    language: str = \"eng\"\n",
                "\n",
                "@dataclass\n",
                "class ObjectDetectionConfig:\n",
                "    \"\"\"Configuration for YOLO object detection.\"\"\"\n",
                "    enabled: bool = True\n",
                "    frame_step: int = 15\n",
                "    confidence_threshold: float = 0.5\n",
                "    nms_threshold: float = 0.4\n",
                "    model_size: str = \"n\"  # n, s, m, l, x\n",
                "    use_gpu: bool = False\n",
                "\n",
                "@dataclass\n",
                "class ExtractorConfig:\n",
                "    \"\"\"Main configuration container.\"\"\"\n",
                "    shot_cut: ShotCutConfig = field(default_factory=ShotCutConfig)\n",
                "    motion: MotionConfig = field(default_factory=MotionConfig)\n",
                "    text_detection: TextDetectionConfig = field(default_factory=TextDetectionConfig)\n",
                "    object_detection: ObjectDetectionConfig = field(default_factory=ObjectDetectionConfig)\n",
                "    log_level: str = \"INFO\"\n",
                "\n",
                "print(\"‚úÖ Configuration classes defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Video utilities defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# VIDEO UTILITIES\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass\n",
                "class VideoMetadata:\n",
                "    \"\"\"Container for video metadata.\"\"\"\n",
                "    path: str\n",
                "    width: int\n",
                "    height: int\n",
                "    fps: float\n",
                "    total_frames: int\n",
                "    duration_seconds: float\n",
                "    codec: str\n",
                "    file_size_bytes: int\n",
                "    \n",
                "    def to_dict(self) -> dict:\n",
                "        return {\n",
                "            \"path\": self.path,\n",
                "            \"resolution\": {\"width\": self.width, \"height\": self.height},\n",
                "            \"fps\": self.fps,\n",
                "            \"total_frames\": self.total_frames,\n",
                "            \"duration_seconds\": round(self.duration_seconds, 2),\n",
                "            \"codec\": self.codec,\n",
                "            \"file_size_bytes\": self.file_size_bytes\n",
                "        }\n",
                "\n",
                "def validate_video_file(video_path: Path) -> Path:\n",
                "    \"\"\"Validate that a video file exists.\"\"\"\n",
                "    path = Path(video_path).resolve()\n",
                "    if not path.is_file():\n",
                "        raise VideoNotFoundError(str(video_path))\n",
                "    return path\n",
                "\n",
                "def get_video_metadata(video_path: Path) -> VideoMetadata:\n",
                "    \"\"\"Extract comprehensive metadata from a video file.\"\"\"\n",
                "    path = validate_video_file(video_path)\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(path))\n",
                "    \n",
                "    try:\n",
                "        width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "        height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "        fps = capture.get(cv2.CAP_PROP_FPS)\n",
                "        total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "        \n",
                "        fourcc = int(capture.get(cv2.CAP_PROP_FOURCC))\n",
                "        codec = \"\".join([chr((fourcc >> 8 * i) & 0xFF) for i in range(4)])\n",
                "        \n",
                "        duration = total_frames / fps if fps > 0 else 0.0\n",
                "        file_size = path.stat().st_size\n",
                "        \n",
                "        return VideoMetadata(\n",
                "            path=str(path),\n",
                "            width=width,\n",
                "            height=height,\n",
                "            fps=fps,\n",
                "            total_frames=total_frames,\n",
                "            duration_seconds=duration,\n",
                "            codec=codec.strip(),\n",
                "            file_size_bytes=file_size\n",
                "        )\n",
                "    finally:\n",
                "        capture.release()\n",
                "\n",
                "print(\"‚úÖ Video utilities defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Shot cut detection defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# SHOT CUT DETECTION\n",
                "# ============================================================================\n",
                "\n",
                "def detect_shot_cuts(\n",
                "    video_path: Path,\n",
                "    config: ShotCutConfig,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Detect hard cuts in video by measuring mean pixel differences.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Path to the video file\n",
                "        config: Shot cut detection configuration\n",
                "        verbose: Whether to print progress\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with shot cut count and metadata\n",
                "    \"\"\"\n",
                "    if verbose:\n",
                "        print(f\"üé¨ Detecting shot cuts...\")\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(video_path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(video_path))\n",
                "    \n",
                "    try:\n",
                "        total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "        \n",
                "        cuts: List[int] = []\n",
                "        frame_idx = 0\n",
                "        last_cut_frame = -config.min_gap_frames\n",
                "        prev_gray = None\n",
                "        \n",
                "        while True:\n",
                "            ok, frame = capture.read()\n",
                "            if not ok:\n",
                "                break\n",
                "            \n",
                "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "            \n",
                "            if prev_gray is not None:\n",
                "                frame_diff = cv2.absdiff(gray, prev_gray)\n",
                "                mean_diff = float(np.mean(frame_diff))\n",
                "                \n",
                "                if mean_diff > config.diff_threshold:\n",
                "                    if (frame_idx - last_cut_frame) >= config.min_gap_frames:\n",
                "                        cuts.append(frame_idx)\n",
                "                        last_cut_frame = frame_idx\n",
                "            \n",
                "            prev_gray = gray\n",
                "            frame_idx += 1\n",
                "            \n",
                "            # Frame stepping\n",
                "            if config.frame_step > 1:\n",
                "                for _ in range(config.frame_step - 1):\n",
                "                    capture.grab()\n",
                "                    frame_idx += 1\n",
                "    finally:\n",
                "        capture.release()\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"   ‚úì Found {len(cuts)} cuts\")\n",
                "    \n",
                "    return {\n",
                "        \"shot_cut_count\": len(cuts),\n",
                "        \"cut_frames\": cuts[:100],\n",
                "        \"frame_step_used\": config.frame_step,\n",
                "        \"mean_diff_threshold\": config.diff_threshold,\n",
                "        \"min_gap_frames\": config.min_gap_frames,\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Shot cut detection defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Motion analysis defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# MOTION ANALYSIS\n",
                "# ============================================================================\n",
                "\n",
                "def analyze_motion(\n",
                "    video_path: Path,\n",
                "    config: MotionConfig,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Analyze motion in video using Farneback optical flow.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Path to the video file\n",
                "        config: Motion analysis configuration\n",
                "        verbose: Whether to print progress\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with motion statistics\n",
                "    \"\"\"\n",
                "    if verbose:\n",
                "        print(f\"üèÉ Analyzing motion...\")\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(video_path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(video_path))\n",
                "    \n",
                "    try:\n",
                "        magnitudes: List[float] = []\n",
                "        prev_gray = None\n",
                "        \n",
                "        while True:\n",
                "            ok, frame = capture.read()\n",
                "            if not ok:\n",
                "                break\n",
                "            \n",
                "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "            \n",
                "            if prev_gray is not None:\n",
                "                flow = cv2.calcOpticalFlowFarneback(\n",
                "                    prev_gray, gray, None,\n",
                "                    config.pyr_scale,\n",
                "                    config.levels,\n",
                "                    config.winsize,\n",
                "                    config.iterations,\n",
                "                    config.poly_n,\n",
                "                    config.poly_sigma,\n",
                "                    0\n",
                "                )\n",
                "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
                "                magnitudes.append(float(np.mean(mag)))\n",
                "            \n",
                "            prev_gray = gray\n",
                "            \n",
                "            # Frame stepping\n",
                "            if config.frame_step > 1:\n",
                "                for _ in range(config.frame_step - 1):\n",
                "                    capture.grab()\n",
                "    finally:\n",
                "        capture.release()\n",
                "    \n",
                "    # Calculate statistics\n",
                "    if magnitudes:\n",
                "        avg_motion = float(np.mean(magnitudes))\n",
                "        max_motion = float(np.max(magnitudes))\n",
                "        min_motion = float(np.min(magnitudes))\n",
                "        std_motion = float(np.std(magnitudes))\n",
                "    else:\n",
                "        avg_motion = max_motion = min_motion = std_motion = 0.0\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"   ‚úì Avg motion: {avg_motion:.4f}\")\n",
                "    \n",
                "    return {\n",
                "        \"average_motion_magnitude\": round(avg_motion, 4),\n",
                "        \"max_motion_magnitude\": round(max_motion, 4),\n",
                "        \"min_motion_magnitude\": round(min_motion, 4),\n",
                "        \"motion_std\": round(std_motion, 4),\n",
                "        \"motion_samples\": len(magnitudes),\n",
                "        \"frame_step_used\": config.frame_step,\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Motion analysis defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Text detection defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# TEXT DETECTION (OCR)\n",
                "# ============================================================================\n",
                "\n",
                "def detect_text(\n",
                "    video_path: Path,\n",
                "    config: TextDetectionConfig,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Detect text in video frames using Tesseract OCR.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Path to the video file\n",
                "        config: Text detection configuration\n",
                "        verbose: Whether to print progress\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with text detection results\n",
                "    \"\"\"\n",
                "    if not TESSERACT_AVAILABLE:\n",
                "        if verbose:\n",
                "            print(\"üìù Text detection: Tesseract not installed, skipping...\")\n",
                "        return {\n",
                "            \"text_present_ratio\": 0.0,\n",
                "            \"frames_with_text\": 0,\n",
                "            \"total_frames_evaluated\": 0,\n",
                "            \"keywords_top10\": [],\n",
                "            \"available\": False,\n",
                "            \"error\": \"Tesseract OCR not installed\"\n",
                "        }\n",
                "    \n",
                "    # Check if tesseract binary is available\n",
                "    try:\n",
                "        pytesseract.get_tesseract_version()\n",
                "    except pytesseract.TesseractNotFoundError:\n",
                "        if verbose:\n",
                "            print(\"üìù Text detection: Tesseract binary not in PATH, skipping...\")\n",
                "        return {\n",
                "            \"text_present_ratio\": 0.0,\n",
                "            \"frames_with_text\": 0,\n",
                "            \"total_frames_evaluated\": 0,\n",
                "            \"keywords_top10\": [],\n",
                "            \"available\": False,\n",
                "            \"error\": \"Tesseract binary not found in PATH\"\n",
                "        }\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"üìù Detecting text (OCR)...\")\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(video_path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(video_path))\n",
                "    \n",
                "    try:\n",
                "        frames_evaluated = 0\n",
                "        frames_with_text = 0\n",
                "        keywords: Counter = Counter()\n",
                "        \n",
                "        while True:\n",
                "            ok, frame = capture.read()\n",
                "            if not ok:\n",
                "                break\n",
                "            \n",
                "            frames_evaluated += 1\n",
                "            \n",
                "            # Preprocess frame\n",
                "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "            blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
                "            _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
                "            \n",
                "            # Run OCR\n",
                "            try:\n",
                "                data = pytesseract.image_to_data(\n",
                "                    binary,\n",
                "                    output_type=pytesseract.Output.DICT,\n",
                "                    lang=config.language\n",
                "                )\n",
                "            except Exception:\n",
                "                continue\n",
                "            \n",
                "            # Process results\n",
                "            has_text = False\n",
                "            for word, conf in zip(data.get(\"text\", []), data.get(\"conf\", [])):\n",
                "                if not word or word.isspace():\n",
                "                    continue\n",
                "                try:\n",
                "                    conf_val = float(conf)\n",
                "                except (ValueError, TypeError):\n",
                "                    continue\n",
                "                \n",
                "                if conf_val >= config.min_confidence:\n",
                "                    has_text = True\n",
                "                    cleaned = word.strip().lower()\n",
                "                    if len(cleaned) >= 2:\n",
                "                        keywords[cleaned] += 1\n",
                "            \n",
                "            if has_text:\n",
                "                frames_with_text += 1\n",
                "            \n",
                "            # Frame stepping\n",
                "            if config.frame_step > 1:\n",
                "                for _ in range(config.frame_step - 1):\n",
                "                    capture.grab()\n",
                "    finally:\n",
                "        capture.release()\n",
                "    \n",
                "    ratio = frames_with_text / frames_evaluated if frames_evaluated > 0 else 0.0\n",
                "    top_keywords = [word for word, _ in keywords.most_common(10)]\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"   ‚úì Text in {frames_with_text}/{frames_evaluated} frames ({ratio:.1%})\")\n",
                "    \n",
                "    return {\n",
                "        \"text_present_ratio\": round(ratio, 4),\n",
                "        \"frames_with_text\": frames_with_text,\n",
                "        \"total_frames_evaluated\": frames_evaluated,\n",
                "        \"keywords_top10\": top_keywords,\n",
                "        \"unique_words\": len(keywords),\n",
                "        \"frame_step_used\": config.frame_step,\n",
                "        \"min_confidence\": config.min_confidence,\n",
                "        \"available\": True,\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Text detection defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Object detection defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# OBJECT/PERSON DETECTION (YOLO)\n",
                "# ============================================================================\n",
                "\n",
                "# Global model cache\n",
                "_yolo_model = None\n",
                "\n",
                "def detect_objects(\n",
                "    video_path: Path,\n",
                "    config: ObjectDetectionConfig,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Detect objects and people in video using YOLOv8.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Path to the video file\n",
                "        config: Object detection configuration\n",
                "        verbose: Whether to print progress\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with object detection results\n",
                "    \"\"\"\n",
                "    global _yolo_model\n",
                "    \n",
                "    if not YOLO_AVAILABLE:\n",
                "        if verbose:\n",
                "            print(\"üéØ Object detection: YOLO not installed, skipping...\")\n",
                "        return {\n",
                "            \"persons_detected\": 0,\n",
                "            \"objects_detected\": 0,\n",
                "            \"person_ratio\": 0.0,\n",
                "            \"object_ratio\": 0.0,\n",
                "            \"dominant_category\": \"unknown\",\n",
                "            \"frames_evaluated\": 0,\n",
                "            \"available\": False,\n",
                "            \"error\": \"ultralytics package not installed\"\n",
                "        }\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"üéØ Detecting objects with YOLOv8...\")\n",
                "    \n",
                "    # Load model (cached)\n",
                "    if _yolo_model is None:\n",
                "        model_name = f\"yolov8{config.model_size}.pt\"\n",
                "        if verbose:\n",
                "            print(f\"   Loading model: {model_name}\")\n",
                "        _yolo_model = YOLO(model_name)\n",
                "    \n",
                "    model = _yolo_model\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(video_path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(video_path))\n",
                "    \n",
                "    try:\n",
                "        persons = 0\n",
                "        objects = 0\n",
                "        class_counts: Dict[str, int] = {}\n",
                "        frames_evaluated = 0\n",
                "        \n",
                "        while True:\n",
                "            ok, frame = capture.read()\n",
                "            if not ok:\n",
                "                break\n",
                "            \n",
                "            frames_evaluated += 1\n",
                "            \n",
                "            # Run YOLO inference\n",
                "            results = model(\n",
                "                frame,\n",
                "                conf=config.confidence_threshold,\n",
                "                verbose=False,\n",
                "                device=\"cuda\" if config.use_gpu else \"cpu\"\n",
                "            )\n",
                "            \n",
                "            # Process detections\n",
                "            for result in results:\n",
                "                boxes = result.boxes\n",
                "                if boxes is None:\n",
                "                    continue\n",
                "                \n",
                "                for box in boxes:\n",
                "                    class_id = int(box.cls[0])\n",
                "                    class_name = model.names[class_id]\n",
                "                    \n",
                "                    class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
                "                    \n",
                "                    if class_id == 0:  # Person class\n",
                "                        persons += 1\n",
                "                    else:\n",
                "                        objects += 1\n",
                "            \n",
                "            # Frame stepping\n",
                "            if config.frame_step > 1:\n",
                "                for _ in range(config.frame_step - 1):\n",
                "                    capture.grab()\n",
                "    finally:\n",
                "        capture.release()\n",
                "    \n",
                "    # Calculate ratios\n",
                "    total = persons + objects\n",
                "    person_ratio = persons / total if total > 0 else 0.0\n",
                "    object_ratio = objects / total if total > 0 else 0.0\n",
                "    \n",
                "    if persons > objects:\n",
                "        dominant = \"person\"\n",
                "    elif objects > persons:\n",
                "        dominant = \"object\"\n",
                "    else:\n",
                "        dominant = \"tie\"\n",
                "    \n",
                "    sorted_classes = dict(sorted(class_counts.items(), key=lambda x: x[1], reverse=True))\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"   ‚úì {persons} persons, {objects} objects detected\")\n",
                "    \n",
                "    return {\n",
                "        \"persons_detected\": persons,\n",
                "        \"objects_detected\": objects,\n",
                "        \"person_ratio\": round(person_ratio, 4),\n",
                "        \"object_ratio\": round(object_ratio, 4),\n",
                "        \"dominant_category\": dominant,\n",
                "        \"class_distribution\": sorted_classes,\n",
                "        \"top_classes\": list(sorted_classes.keys())[:10],\n",
                "        \"frames_evaluated\": frames_evaluated,\n",
                "        \"frame_step_used\": config.frame_step,\n",
                "        \"confidence_threshold\": config.confidence_threshold,\n",
                "        \"model_used\": f\"yolov8{config.model_size}\",\n",
                "        \"available\": True,\n",
                "    }\n",
                "\n",
                "print(\"‚úÖ Object detection defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ VideoFeatureExtractor class defined\n",
                        "\n",
                        "üéâ All components loaded! Ready to extract features.\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# MAIN EXTRACTOR CLASS\n",
                "# ============================================================================\n",
                "\n",
                "class VideoFeatureExtractor:\n",
                "    \"\"\"\n",
                "    Main class for extracting features from video files.\n",
                "    \n",
                "    Example:\n",
                "        extractor = VideoFeatureExtractor()\n",
                "        results = extractor.extract(\"video.mp4\", features=[\"cuts\", \"motion\"])\n",
                "    \"\"\"\n",
                "    \n",
                "    VERSION = \"2.0.0\"\n",
                "    AVAILABLE_FEATURES = {\"cuts\", \"motion\", \"text\", \"objects\"}\n",
                "    \n",
                "    def __init__(self, config: ExtractorConfig = None):\n",
                "        self.config = config or ExtractorConfig()\n",
                "    \n",
                "    def check_availability(self) -> Dict[str, bool]:\n",
                "        \"\"\"Check which features are available.\"\"\"\n",
                "        return {\n",
                "            \"cuts\": True,  # Always available (OpenCV)\n",
                "            \"motion\": True,  # Always available (OpenCV)\n",
                "            \"text\": TESSERACT_AVAILABLE and self._check_tesseract(),\n",
                "            \"objects\": YOLO_AVAILABLE,\n",
                "        }\n",
                "    \n",
                "    def _check_tesseract(self) -> bool:\n",
                "        \"\"\"Check if Tesseract binary is available.\"\"\"\n",
                "        if not TESSERACT_AVAILABLE:\n",
                "            return False\n",
                "        try:\n",
                "            pytesseract.get_tesseract_version()\n",
                "            return True\n",
                "        except:\n",
                "            return False\n",
                "    \n",
                "    def extract(\n",
                "        self,\n",
                "        video_path: str | Path,\n",
                "        features: List[str] = None,\n",
                "        include_metadata: bool = True,\n",
                "        verbose: bool = True\n",
                "    ) -> Dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Extract features from a video file.\n",
                "        \n",
                "        Args:\n",
                "            video_path: Path to the video file\n",
                "            features: List of features to extract (default: all available)\n",
                "            include_metadata: Whether to include video metadata\n",
                "            verbose: Whether to print progress\n",
                "            \n",
                "        Returns:\n",
                "            Dictionary with all extraction results\n",
                "        \"\"\"\n",
                "        start_time = datetime.now()\n",
                "        video_path = Path(video_path)\n",
                "        \n",
                "        # Validate video\n",
                "        video_path = validate_video_file(video_path)\n",
                "        \n",
                "        # Determine features\n",
                "        if features is None:\n",
                "            features = list(self.AVAILABLE_FEATURES)\n",
                "        \n",
                "        # Validate features\n",
                "        invalid = set(features) - self.AVAILABLE_FEATURES\n",
                "        if invalid:\n",
                "            raise InvalidFeatureError(features, self.AVAILABLE_FEATURES)\n",
                "        \n",
                "        if verbose:\n",
                "            print(f\"\\n{'='*60}\")\n",
                "            print(f\"üé¨ Video Feature Extraction v{self.VERSION}\")\n",
                "            print(f\"{'='*60}\")\n",
                "            print(f\"üìÅ Video: {video_path.name}\")\n",
                "            print(f\"üìã Features: {', '.join(features)}\")\n",
                "            print(f\"{'='*60}\\n\")\n",
                "        \n",
                "        # Initialize output\n",
                "        output: Dict[str, Any] = {\n",
                "            \"video_path\": str(video_path),\n",
                "            \"extraction_timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
                "            \"features_requested\": features,\n",
                "            \"extractor_version\": self.VERSION,\n",
                "        }\n",
                "        \n",
                "        # Add metadata\n",
                "        if include_metadata:\n",
                "            try:\n",
                "                metadata = get_video_metadata(video_path)\n",
                "                output[\"video_metadata\"] = metadata.to_dict()\n",
                "            except Exception as e:\n",
                "                output[\"video_metadata\"] = {\"error\": str(e)}\n",
                "        \n",
                "        # Extract features\n",
                "        results: Dict[str, Any] = {}\n",
                "        \n",
                "        if \"cuts\" in features:\n",
                "            results[\"shot_cut_detection\"] = detect_shot_cuts(\n",
                "                video_path, self.config.shot_cut, verbose\n",
                "            )\n",
                "        \n",
                "        if \"motion\" in features:\n",
                "            results[\"motion_analysis\"] = analyze_motion(\n",
                "                video_path, self.config.motion, verbose\n",
                "            )\n",
                "        \n",
                "        if \"text\" in features:\n",
                "            results[\"text_detection\"] = detect_text(\n",
                "                video_path, self.config.text_detection, verbose\n",
                "            )\n",
                "        \n",
                "        if \"objects\" in features:\n",
                "            results[\"object_person_dominance\"] = detect_objects(\n",
                "                video_path, self.config.object_detection, verbose\n",
                "            )\n",
                "        \n",
                "        output[\"results\"] = results\n",
                "        \n",
                "        # Calculate processing time\n",
                "        elapsed = (datetime.now() - start_time).total_seconds()\n",
                "        output[\"processing_time_seconds\"] = round(elapsed, 2)\n",
                "        \n",
                "        if verbose:\n",
                "            print(f\"\\n{'='*60}\")\n",
                "            print(f\"‚úÖ Extraction complete in {elapsed:.2f} seconds\")\n",
                "            print(f\"{'='*60}\")\n",
                "        \n",
                "        return output\n",
                "    \n",
                "    def extract_to_json(\n",
                "        self,\n",
                "        video_path: str | Path,\n",
                "        output_path: str | Path = None,\n",
                "        features: List[str] = None,\n",
                "        pretty: bool = True,\n",
                "        verbose: bool = True\n",
                "    ) -> str:\n",
                "        \"\"\"Extract features and return/save as JSON.\"\"\"\n",
                "        results = self.extract(video_path, features, verbose=verbose)\n",
                "        \n",
                "        indent = 2 if pretty else None\n",
                "        json_output = json.dumps(results, indent=indent, default=str)\n",
                "        \n",
                "        if output_path:\n",
                "            Path(output_path).write_text(json_output)\n",
                "            if verbose:\n",
                "                print(f\"üíæ Saved to: {output_path}\")\n",
                "        \n",
                "        return json_output\n",
                "\n",
                "print(\"‚úÖ VideoFeatureExtractor class defined\")\n",
                "print(f\"\\nüéâ All components loaded! Ready to extract features.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Usage Examples\n",
                "\n",
                "Now let's use the extractor to analyze videos!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìã Feature Availability:\n",
                        "----------------------------------------\n",
                        "   cuts       ‚úÖ Available\n",
                        "   motion     ‚úÖ Available\n",
                        "   text       ‚ùå Not available\n",
                        "   objects    ‚úÖ Available\n"
                    ]
                }
            ],
            "source": [
                "# Create extractor instance\n",
                "extractor = VideoFeatureExtractor()\n",
                "\n",
                "# Check feature availability\n",
                "print(\"üìã Feature Availability:\")\n",
                "print(\"-\" * 40)\n",
                "for feature, available in extractor.check_availability().items():\n",
                "    status = \"‚úÖ Available\" if available else \"‚ùå Not available\"\n",
                "    print(f\"   {feature:10} {status}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Video found: videoplayback.mp4\n",
                        "\n",
                        "üìπ Video Info:\n",
                        "   Resolution: 360x640\n",
                        "   Duration:   11.70s\n",
                        "   FPS:        30.0\n",
                        "   Frames:     351\n"
                    ]
                }
            ],
            "source": [
                "# Set video path - UPDATE THIS TO YOUR VIDEO\n",
                "VIDEO_PATH = Path(\"videoplayback.mp4\")\n",
                "\n",
                "if VIDEO_PATH.exists():\n",
                "    print(f\"‚úÖ Video found: {VIDEO_PATH}\")\n",
                "    \n",
                "    # Get metadata\n",
                "    metadata = get_video_metadata(VIDEO_PATH)\n",
                "    print(f\"\\nüìπ Video Info:\")\n",
                "    print(f\"   Resolution: {metadata.width}x{metadata.height}\")\n",
                "    print(f\"   Duration:   {metadata.duration_seconds:.2f}s\")\n",
                "    print(f\"   FPS:        {metadata.fps}\")\n",
                "    print(f\"   Frames:     {metadata.total_frames}\")\n",
                "else:\n",
                "    print(f\"‚ùå Video not found: {VIDEO_PATH}\")\n",
                "    print(\"   Please update VIDEO_PATH to point to your video file.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Extract Shot Cuts and Motion (Fast)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "üé¨ Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "üìÅ Video: videoplayback.mp4\n",
                        "üìã Features: cuts, motion\n",
                        "============================================================\n",
                        "\n",
                        "üé¨ Detecting shot cuts...\n",
                        "   ‚úì Found 0 cuts\n",
                        "üèÉ Analyzing motion...\n",
                        "   ‚úì Avg motion: 0.9316\n",
                        "\n",
                        "============================================================\n",
                        "‚úÖ Extraction complete in 2.64 seconds\n",
                        "============================================================\n",
                        "\n",
                        "üìä RESULTS:\n",
                        "----------------------------------------\n",
                        "\n",
                        "üé¨ Shot Cuts:\n",
                        "   Count: 0\n",
                        "\n",
                        "üèÉ Motion:\n",
                        "   Average: 0.9316\n",
                        "   Max:     2.8623\n",
                        "   Std Dev: 0.5769\n"
                    ]
                }
            ],
            "source": [
                "# Extract fast features\n",
                "if VIDEO_PATH.exists():\n",
                "    results = extractor.extract(\n",
                "        VIDEO_PATH,\n",
                "        features=[\"cuts\", \"motion\"]\n",
                "    )\n",
                "    \n",
                "    # Display results\n",
                "    print(\"\\nüìä RESULTS:\")\n",
                "    print(\"-\" * 40)\n",
                "    \n",
                "    cuts = results[\"results\"][\"shot_cut_detection\"]\n",
                "    print(f\"\\nüé¨ Shot Cuts:\")\n",
                "    print(f\"   Count: {cuts['shot_cut_count']}\")\n",
                "    \n",
                "    motion = results[\"results\"][\"motion_analysis\"]\n",
                "    print(f\"\\nüèÉ Motion:\")\n",
                "    print(f\"   Average: {motion['average_motion_magnitude']:.4f}\")\n",
                "    print(f\"   Max:     {motion['max_motion_magnitude']:.4f}\")\n",
                "    print(f\"   Std Dev: {motion['motion_std']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Object Detection with YOLOv8"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "üé¨ Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "üìÅ Video: videoplayback.mp4\n",
                        "üìã Features: objects\n",
                        "============================================================\n",
                        "\n",
                        "üéØ Detecting objects with YOLOv8...\n",
                        "   Loading model: yolov8n.pt\n",
                        "   ‚úì 24 persons, 0 objects detected\n",
                        "\n",
                        "============================================================\n",
                        "‚úÖ Extraction complete in 0.57 seconds\n",
                        "============================================================\n",
                        "\n",
                        "üéØ Object Detection Results:\n",
                        "----------------------------------------\n",
                        "   Persons:  24\n",
                        "   Objects:  0\n",
                        "   Dominant: PERSON\n",
                        "\n",
                        "   üì¶ Top Classes:\n",
                        "      person: 24\n"
                    ]
                }
            ],
            "source": [
                "# Extract objects (may download model on first run)\n",
                "if VIDEO_PATH.exists():\n",
                "    object_results = extractor.extract(\n",
                "        VIDEO_PATH,\n",
                "        features=[\"objects\"]\n",
                "    )\n",
                "    \n",
                "    obj = object_results[\"results\"][\"object_person_dominance\"]\n",
                "    \n",
                "    if obj.get(\"available\", True):\n",
                "        print(\"\\nüéØ Object Detection Results:\")\n",
                "        print(\"-\" * 40)\n",
                "        print(f\"   Persons:  {obj['persons_detected']}\")\n",
                "        print(f\"   Objects:  {obj['objects_detected']}\")\n",
                "        print(f\"   Dominant: {obj['dominant_category'].upper()}\")\n",
                "        \n",
                "        if obj.get('class_distribution'):\n",
                "            print(\"\\n   üì¶ Top Classes:\")\n",
                "            for cls, count in list(obj['class_distribution'].items())[:5]:\n",
                "                print(f\"      {cls}: {count}\")\n",
                "    else:\n",
                "        print(f\"   ‚ö†Ô∏è {obj.get('error', 'Not available')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Custom Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Custom configuration created\n",
                        "   Shot cut threshold: 20.0\n",
                        "   Motion frame step:  5\n",
                        "   Object confidence:  0.7\n"
                    ]
                }
            ],
            "source": [
                "# Create custom configuration\n",
                "custom_config = ExtractorConfig()\n",
                "\n",
                "# More sensitive shot cut detection\n",
                "custom_config.shot_cut.diff_threshold = 20.0  # Lower = more sensitive\n",
                "\n",
                "# Faster motion analysis\n",
                "custom_config.motion.frame_step = 5  # Skip more frames\n",
                "\n",
                "# Higher confidence for objects\n",
                "custom_config.object_detection.confidence_threshold = 0.7\n",
                "\n",
                "# Create new extractor with custom config\n",
                "custom_extractor = VideoFeatureExtractor(custom_config)\n",
                "\n",
                "print(\"‚úÖ Custom configuration created\")\n",
                "print(f\"   Shot cut threshold: {custom_config.shot_cut.diff_threshold}\")\n",
                "print(f\"   Motion frame step:  {custom_config.motion.frame_step}\")\n",
                "print(f\"   Object confidence:  {custom_config.object_detection.confidence_threshold}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "üé¨ Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "üìÅ Video: videoplayback.mp4\n",
                        "üìã Features: cuts, motion\n",
                        "============================================================\n",
                        "\n",
                        "üé¨ Detecting shot cuts...\n",
                        "   ‚úì Found 0 cuts\n",
                        "üèÉ Analyzing motion...\n",
                        "   ‚úì Avg motion: 1.9809\n",
                        "\n",
                        "============================================================\n",
                        "‚úÖ Extraction complete in 1.09 seconds\n",
                        "============================================================\n",
                        "\n",
                        "üìä Custom Config Results:\n",
                        "   Cuts found: 0\n",
                        "   Motion avg: 1.9809\n"
                    ]
                }
            ],
            "source": [
                "# Run with custom config\n",
                "if VIDEO_PATH.exists():\n",
                "    custom_results = custom_extractor.extract(\n",
                "        VIDEO_PATH,\n",
                "        features=[\"cuts\", \"motion\"]\n",
                "    )\n",
                "    \n",
                "    print(\"\\nüìä Custom Config Results:\")\n",
                "    print(f\"   Cuts found: {custom_results['results']['shot_cut_detection']['shot_cut_count']}\")\n",
                "    print(f\"   Motion avg: {custom_results['results']['motion_analysis']['average_motion_magnitude']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Export to JSON"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "üé¨ Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "üìÅ Video: videoplayback.mp4\n",
                        "üìã Features: cuts, motion\n",
                        "============================================================\n",
                        "\n",
                        "üé¨ Detecting shot cuts...\n",
                        "   ‚úì Found 0 cuts\n",
                        "üèÉ Analyzing motion...\n",
                        "   ‚úì Avg motion: 0.9316\n",
                        "\n",
                        "============================================================\n",
                        "‚úÖ Extraction complete in 2.55 seconds\n",
                        "============================================================\n",
                        "üíæ Saved to: video_analysis_results.json\n",
                        "\n",
                        "üìÑ JSON Preview (first 500 chars):\n",
                        "{\n",
                        "  \"video_path\": \"/Users/ridam/Desktop/White Panda/videoplayback.mp4\",\n",
                        "  \"extraction_timestamp\": \"2026-01-14T09:38:57.626928Z\",\n",
                        "  \"features_requested\": [\n",
                        "    \"cuts\",\n",
                        "    \"motion\"\n",
                        "  ],\n",
                        "  \"extractor_version\": \"2.0.0\",\n",
                        "  \"video_metadata\": {\n",
                        "    \"path\": \"/Users/ridam/Desktop/White Panda/videoplayback.mp4\",\n",
                        "    \"resolution\": {\n",
                        "      \"width\": 360,\n",
                        "      \"height\": 640\n",
                        "    },\n",
                        "    \"fps\": 30.0,\n",
                        "    \"total_frames\": 351,\n",
                        "    \"duration_seconds\": 11.7,\n",
                        "    \"codec\": \"h264\",\n",
                        "    \"file_size_bytes\": 877848\n",
                        "  },\n",
                        "...\n"
                    ]
                }
            ],
            "source": [
                "# Export full results to JSON\n",
                "if VIDEO_PATH.exists():\n",
                "    output_file = \"video_analysis_results.json\"\n",
                "    \n",
                "    json_output = extractor.extract_to_json(\n",
                "        VIDEO_PATH,\n",
                "        output_path=output_file,\n",
                "        features=[\"cuts\", \"motion\"],\n",
                "        pretty=True\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nüìÑ JSON Preview (first 500 chars):\")\n",
                "    print(json_output[:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5 Full Extraction (All Features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running features: ['cuts', 'motion', 'objects']\n",
                        "\n",
                        "============================================================\n",
                        "üé¨ Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "üìÅ Video: videoplayback.mp4\n",
                        "üìã Features: cuts, motion, objects\n",
                        "============================================================\n",
                        "\n",
                        "üé¨ Detecting shot cuts...\n",
                        "   ‚úì Found 0 cuts\n",
                        "üèÉ Analyzing motion...\n",
                        "   ‚úì Avg motion: 0.9316\n",
                        "üéØ Detecting objects with YOLOv8...\n",
                        "   ‚úì 24 persons, 0 objects detected\n",
                        "\n",
                        "============================================================\n",
                        "‚úÖ Extraction complete in 3.01 seconds\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "# Run all available features\n",
                "if VIDEO_PATH.exists():\n",
                "    availability = extractor.check_availability()\n",
                "    available_features = [f for f, avail in availability.items() if avail]\n",
                "    \n",
                "    print(f\"Running features: {available_features}\")\n",
                "    \n",
                "    full_results = extractor.extract(\n",
                "        VIDEO_PATH,\n",
                "        features=available_features\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "üìä COMPLETE EXTRACTION SUMMARY\n",
                        "============================================================\n",
                        "\n",
                        "üìÅ Video: videoplayback.mp4\n",
                        "‚è±Ô∏è  Time:  3.01s\n",
                        "\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "üìã Shot Cut Detection:\n",
                        "   shot_cut_count: 0\n",
                        "   cut_frames: 0 items\n",
                        "   frame_step_used: 1\n",
                        "   mean_diff_threshold: 30.0000\n",
                        "   min_gap_frames: 5\n",
                        "\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "üìã Motion Analysis:\n",
                        "   average_motion_magnitude: 0.9316\n",
                        "   max_motion_magnitude: 2.8623\n",
                        "   min_motion_magnitude: 0.2001\n",
                        "   motion_std: 0.5769\n",
                        "   motion_samples: 175\n",
                        "\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "üìã Object Person Dominance:\n",
                        "   persons_detected: 24\n",
                        "   objects_detected: 0\n",
                        "   person_ratio: 1.0000\n",
                        "   object_ratio: 0.0000\n",
                        "   dominant_category: person\n"
                    ]
                }
            ],
            "source": [
                "# Display comprehensive summary\n",
                "if VIDEO_PATH.exists():\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"üìä COMPLETE EXTRACTION SUMMARY\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    print(f\"\\nüìÅ Video: {full_results['video_path'].split('/')[-1]}\")\n",
                "    print(f\"‚è±Ô∏è  Time:  {full_results['processing_time_seconds']}s\")\n",
                "    \n",
                "    for key, data in full_results['results'].items():\n",
                "        print(f\"\\n{'‚îÄ' * 50}\")\n",
                "        print(f\"üìã {key.replace('_', ' ').title()}:\")\n",
                "        \n",
                "        if 'error' in data:\n",
                "            print(f\"   ‚ö†Ô∏è {data['error']}\")\n",
                "        else:\n",
                "            for k, v in list(data.items())[:5]:\n",
                "                if isinstance(v, float):\n",
                "                    print(f\"   {k}: {v:.4f}\")\n",
                "                elif isinstance(v, list):\n",
                "                    print(f\"   {k}: {len(v)} items\")\n",
                "                elif isinstance(v, dict):\n",
                "                    print(f\"   {k}: {len(v)} entries\")\n",
                "                else:\n",
                "                    print(f\"   {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Error Handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ VideoNotFoundError caught: Video file not found: /nonexistent/video.mp4\n",
                        "‚úÖ InvalidFeatureError caught: Invalid feature(s): {'invalid'}. Valid: {'objects', 'motion', 'cuts', 'text'}\n"
                    ]
                }
            ],
            "source": [
                "# Example: Handle missing video\n",
                "try:\n",
                "    extractor.extract(\"/nonexistent/video.mp4\", verbose=False)\n",
                "except VideoNotFoundError as e:\n",
                "    print(f\"‚úÖ VideoNotFoundError caught: {e.message}\")\n",
                "\n",
                "# Example: Handle invalid feature\n",
                "try:\n",
                "    extractor.extract(VIDEO_PATH, features=[\"invalid\"], verbose=False)\n",
                "except InvalidFeatureError as e:\n",
                "    print(f\"‚úÖ InvalidFeatureError caught: {e.message}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üóëÔ∏è Removed: video_analysis_results.json\n",
                        "\n",
                        "‚úÖ Cleanup complete!\n"
                    ]
                }
            ],
            "source": [
                "# Clean up temporary files\n",
                "import os\n",
                "\n",
                "temp_files = [\"video_analysis_results.json\"]\n",
                "\n",
                "for f in temp_files:\n",
                "    if Path(f).exists():\n",
                "        os.remove(f)\n",
                "        print(f\"üóëÔ∏è Removed: {f}\")\n",
                "\n",
                "print(\"\\n‚úÖ Cleanup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìö Quick Reference\n",
                "\n",
                "### Available Features\n",
                "| Feature | Description | Requirements |\n",
                "|---------|-------------|-------------|\n",
                "| `cuts` | Shot cut detection | OpenCV |\n",
                "| `motion` | Motion analysis | OpenCV |\n",
                "| `text` | OCR text detection | pytesseract + Tesseract |\n",
                "| `objects` | Object/person detection | ultralytics (YOLOv8) |\n",
                "\n",
                "### Configuration Options\n",
                "```python\n",
                "config = ExtractorConfig()\n",
                "\n",
                "# Shot cut settings\n",
                "config.shot_cut.diff_threshold = 30.0  # Lower = more sensitive\n",
                "config.shot_cut.min_gap_frames = 5\n",
                "\n",
                "# Motion settings\n",
                "config.motion.frame_step = 2  # Higher = faster\n",
                "\n",
                "# Object detection settings\n",
                "config.object_detection.confidence_threshold = 0.5\n",
                "config.object_detection.model_size = \"n\"  # n, s, m, l, x\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
