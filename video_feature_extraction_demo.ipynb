{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83c\udfac Video Feature Extraction Tool\n",
                "\n",
                "**All-in-One Notebook** - Complete video analysis solution with all code embedded.\n",
                "\n",
                "## Features:\n",
                "- **Shot Cut Detection**: Counts hard cuts using frame-to-frame pixel analysis\n",
                "- **Motion Analysis**: Computes average motion magnitude via optical flow\n",
                "- **Text Detection (OCR)**: Detects text presence and extracts keywords\n",
                "- **Object/Person Detection**: Estimates person vs object dominance using YOLOv8\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies\n",
                "\n",
                "Run this cell once to install required packages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment and run to install dependencies\n",
                "# !pip install numpy opencv-python pillow pytesseract ultralytics pyyaml tqdm lapx -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Core Implementation\n",
                "\n",
                "All the video feature extraction code is embedded below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Core imports loaded\n",
                        "   OpenCV: 4.12.0\n",
                        "   Tesseract: Available\n",
                        "   YOLO: Available\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# IMPORTS\n",
                "# ============================================================================\n",
                "\n",
                "import json\n",
                "import logging\n",
                "import sys\n",
                "from abc import ABC, abstractmethod\n",
                "from collections import Counter\n",
                "from dataclasses import dataclass, field\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "\n",
                "# Optional imports\n",
                "try:\n",
                "    import pytesseract\n",
                "    TESSERACT_AVAILABLE = True\n",
                "except ImportError:\n",
                "    TESSERACT_AVAILABLE = False\n",
                "    pytesseract = None\n",
                "\n",
                "try:\n",
                "    from ultralytics import YOLO\n",
                "    YOLO_AVAILABLE = True\n",
                "except ImportError:\n",
                "    YOLO_AVAILABLE = False\n",
                "    YOLO = None\n",
                "\n",
                "print(\"\u2705 Core imports loaded\")\n",
                "print(f\"   OpenCV: {cv2.__version__}\")\n",
                "print(f\"   Tesseract: {'Available' if TESSERACT_AVAILABLE else 'Not installed'}\")\n",
                "print(f\"   YOLO: {'Available' if YOLO_AVAILABLE else 'Not installed'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Exception classes defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# CUSTOM EXCEPTIONS\n",
                "# ============================================================================\n",
                "\n",
                "class VideoFeatureExtractorError(Exception):\n",
                "    \"\"\"Base exception for all Video Feature Extractor errors.\"\"\"\n",
                "    def __init__(self, message: str, details: dict = None):\n",
                "        super().__init__(message)\n",
                "        self.message = message\n",
                "        self.details = details or {}\n",
                "\n",
                "class VideoNotFoundError(VideoFeatureExtractorError):\n",
                "    \"\"\"Raised when the specified video file does not exist.\"\"\"\n",
                "    def __init__(self, video_path: str):\n",
                "        super().__init__(f\"Video file not found: {video_path}\")\n",
                "        self.video_path = video_path\n",
                "\n",
                "class VideoOpenError(VideoFeatureExtractorError):\n",
                "    \"\"\"Raised when a video file cannot be opened.\"\"\"\n",
                "    def __init__(self, video_path: str, reason: str = None):\n",
                "        message = f\"Unable to open video: {video_path}\"\n",
                "        if reason:\n",
                "            message += f\" - {reason}\"\n",
                "        super().__init__(message)\n",
                "        self.video_path = video_path\n",
                "\n",
                "class InvalidFeatureError(VideoFeatureExtractorError):\n",
                "    \"\"\"Raised when an invalid feature name is requested.\"\"\"\n",
                "    def __init__(self, requested: list, valid: set):\n",
                "        invalid = set(requested) - valid\n",
                "        super().__init__(f\"Invalid feature(s): {invalid}. Valid: {valid}\")\n",
                "\n",
                "print(\"\u2705 Exception classes defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Configuration classes defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass\n",
                "class ShotCutConfig:\n",
                "    \"\"\"Configuration for shot cut detection.\"\"\"\n",
                "    enabled: bool = True\n",
                "    frame_step: int = 1\n",
                "    diff_threshold: float = 30.0\n",
                "    min_gap_frames: int = 5\n",
                "\n",
                "@dataclass\n",
                "class MotionConfig:\n",
                "    \"\"\"Configuration for motion analysis.\"\"\"\n",
                "    enabled: bool = True\n",
                "    frame_step: int = 2\n",
                "    pyr_scale: float = 0.5\n",
                "    levels: int = 3\n",
                "    winsize: int = 15\n",
                "    iterations: int = 3\n",
                "    poly_n: int = 5\n",
                "    poly_sigma: float = 1.2\n",
                "\n",
                "@dataclass\n",
                "class TextDetectionConfig:\n",
                "    \"\"\"Configuration for OCR text detection.\"\"\"\n",
                "    enabled: bool = True\n",
                "    frame_step: int = 15\n",
                "    min_confidence: float = 70.0\n",
                "    language: str = \"eng\"\n",
                "\n",
                "@dataclass\n",
                "class ObjectDetectionConfig:\n",
                "    \"\"\"Configuration for YOLO object detection.\"\"\"\n",
                "    enabled: bool = True\n",
                "    frame_step: int = 15\n",
                "    confidence_threshold: float = 0.5\n",
                "    nms_threshold: float = 0.4\n",
                "    model_size: str = \"n\"  # n, s, m, l, x\n",
                "    use_gpu: bool = False\n",
                "\n",
                "@dataclass\n",
                "class ExtractorConfig:\n",
                "    \"\"\"Main configuration container.\"\"\"\n",
                "    shot_cut: ShotCutConfig = field(default_factory=ShotCutConfig)\n",
                "    motion: MotionConfig = field(default_factory=MotionConfig)\n",
                "    text_detection: TextDetectionConfig = field(default_factory=TextDetectionConfig)\n",
                "    object_detection: ObjectDetectionConfig = field(default_factory=ObjectDetectionConfig)\n",
                "    log_level: str = \"INFO\"\n",
                "\n",
                "print(\"\u2705 Configuration classes defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Video utilities defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# VIDEO UTILITIES\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass\n",
                "class VideoMetadata:\n",
                "    \"\"\"Container for video metadata.\"\"\"\n",
                "    path: str\n",
                "    width: int\n",
                "    height: int\n",
                "    fps: float\n",
                "    total_frames: int\n",
                "    duration_seconds: float\n",
                "    codec: str\n",
                "    file_size_bytes: int\n",
                "    \n",
                "    def to_dict(self) -> dict:\n",
                "        return {\n",
                "            \"path\": self.path,\n",
                "            \"resolution\": {\"width\": self.width, \"height\": self.height},\n",
                "            \"fps\": self.fps,\n",
                "            \"total_frames\": self.total_frames,\n",
                "            \"duration_seconds\": round(self.duration_seconds, 2),\n",
                "            \"codec\": self.codec,\n",
                "            \"file_size_bytes\": self.file_size_bytes\n",
                "        }\n",
                "\n",
                "def validate_video_file(video_path: Path) -> Path:\n",
                "    \"\"\"Validate that a video file exists.\"\"\"\n",
                "    path = Path(video_path).resolve()\n",
                "    if not path.is_file():\n",
                "        raise VideoNotFoundError(str(video_path))\n",
                "    return path\n",
                "\n",
                "def get_video_metadata(video_path: Path) -> VideoMetadata:\n",
                "    \"\"\"Extract comprehensive metadata from a video file.\"\"\"\n",
                "    path = validate_video_file(video_path)\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(path))\n",
                "    \n",
                "    try:\n",
                "        width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "        height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "        fps = capture.get(cv2.CAP_PROP_FPS)\n",
                "        total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "        \n",
                "        fourcc = int(capture.get(cv2.CAP_PROP_FOURCC))\n",
                "        codec = \"\".join([chr((fourcc >> 8 * i) & 0xFF) for i in range(4)])\n",
                "        \n",
                "        duration = total_frames / fps if fps > 0 else 0.0\n",
                "        file_size = path.stat().st_size\n",
                "        \n",
                "        return VideoMetadata(\n",
                "            path=str(path),\n",
                "            width=width,\n",
                "            height=height,\n",
                "            fps=fps,\n",
                "            total_frames=total_frames,\n",
                "            duration_seconds=duration,\n",
                "            codec=codec.strip(),\n",
                "            file_size_bytes=file_size\n",
                "        )\n",
                "    finally:\n",
                "        capture.release()\n",
                "\n",
                "print(\"\u2705 Video utilities defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Shot cut detection defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# SHOT CUT DETECTION\n",
                "# ============================================================================\n",
                "\n",
                "def detect_shot_cuts(\n",
                "    video_path: Path,\n",
                "    config: ShotCutConfig,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Detect hard cuts in video by measuring mean pixel differences.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Path to the video file\n",
                "        config: Shot cut detection configuration\n",
                "        verbose: Whether to print progress\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with shot cut count and metadata\n",
                "    \"\"\"\n",
                "    if verbose:\n",
                "        print(f\"\ud83c\udfac Detecting shot cuts...\")\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(video_path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(video_path))\n",
                "    \n",
                "    try:\n",
                "        total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "        \n",
                "        cuts: List[int] = []\n",
                "        frame_idx = 0\n",
                "        last_cut_frame = -config.min_gap_frames\n",
                "        prev_gray = None\n",
                "        \n",
                "        while True:\n",
                "            ok, frame = capture.read()\n",
                "            if not ok:\n",
                "                break\n",
                "            \n",
                "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "            \n",
                "            if prev_gray is not None:\n",
                "                frame_diff = cv2.absdiff(gray, prev_gray)\n",
                "                mean_diff = float(np.mean(frame_diff))\n",
                "                \n",
                "                if mean_diff > config.diff_threshold:\n",
                "                    if (frame_idx - last_cut_frame) >= config.min_gap_frames:\n",
                "                        cuts.append(frame_idx)\n",
                "                        last_cut_frame = frame_idx\n",
                "            \n",
                "            prev_gray = gray\n",
                "            frame_idx += 1\n",
                "            \n",
                "            # Frame stepping\n",
                "            if config.frame_step > 1:\n",
                "                for _ in range(config.frame_step - 1):\n",
                "                    capture.grab()\n",
                "                    frame_idx += 1\n",
                "    finally:\n",
                "        capture.release()\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"   \u2713 Found {len(cuts)} cuts\")\n",
                "    \n",
                "    return {\n",
                "        \"shot_cut_count\": len(cuts),\n",
                "        \"cut_frames\": cuts[:100],\n",
                "        \"frame_step_used\": config.frame_step,\n",
                "        \"mean_diff_threshold\": config.diff_threshold,\n",
                "        \"min_gap_frames\": config.min_gap_frames,\n",
                "    }\n",
                "\n",
                "print(\"\u2705 Shot cut detection defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Motion analysis defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# MOTION ANALYSIS\n",
                "# ============================================================================\n",
                "\n",
                "def analyze_motion(\n",
                "    video_path: Path,\n",
                "    config: MotionConfig,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Analyze motion in video using Farneback optical flow.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Path to the video file\n",
                "        config: Motion analysis configuration\n",
                "        verbose: Whether to print progress\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with motion statistics\n",
                "    \"\"\"\n",
                "    if verbose:\n",
                "        print(f\"\ud83c\udfc3 Analyzing motion...\")\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(video_path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(video_path))\n",
                "    \n",
                "    try:\n",
                "        magnitudes: List[float] = []\n",
                "        prev_gray = None\n",
                "        \n",
                "        while True:\n",
                "            ok, frame = capture.read()\n",
                "            if not ok:\n",
                "                break\n",
                "            \n",
                "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "            \n",
                "            if prev_gray is not None:\n",
                "                flow = cv2.calcOpticalFlowFarneback(\n",
                "                    prev_gray, gray, None,\n",
                "                    config.pyr_scale,\n",
                "                    config.levels,\n",
                "                    config.winsize,\n",
                "                    config.iterations,\n",
                "                    config.poly_n,\n",
                "                    config.poly_sigma,\n",
                "                    0\n",
                "                )\n",
                "                mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
                "                magnitudes.append(float(np.mean(mag)))\n",
                "            \n",
                "            prev_gray = gray\n",
                "            \n",
                "            # Frame stepping\n",
                "            if config.frame_step > 1:\n",
                "                for _ in range(config.frame_step - 1):\n",
                "                    capture.grab()\n",
                "    finally:\n",
                "        capture.release()\n",
                "    \n",
                "    # Calculate statistics\n",
                "    if magnitudes:\n",
                "        avg_motion = float(np.mean(magnitudes))\n",
                "        max_motion = float(np.max(magnitudes))\n",
                "        min_motion = float(np.min(magnitudes))\n",
                "        std_motion = float(np.std(magnitudes))\n",
                "    else:\n",
                "        avg_motion = max_motion = min_motion = std_motion = 0.0\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"   \u2713 Avg motion: {avg_motion:.4f}\")\n",
                "    \n",
                "    return {\n",
                "        \"average_motion_magnitude\": round(avg_motion, 4),\n",
                "        \"max_motion_magnitude\": round(max_motion, 4),\n",
                "        \"min_motion_magnitude\": round(min_motion, 4),\n",
                "        \"motion_std\": round(std_motion, 4),\n",
                "        \"motion_samples\": len(magnitudes),\n",
                "        \"frame_step_used\": config.frame_step,\n",
                "    }\n",
                "\n",
                "print(\"\u2705 Motion analysis defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Text detection defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# TEXT DETECTION (OCR)\n",
                "# ============================================================================\n",
                "\n",
                "def detect_text(\n",
                "    video_path: Path,\n",
                "    config: TextDetectionConfig,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Detect text in video frames using Tesseract OCR.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Path to the video file\n",
                "        config: Text detection configuration\n",
                "        verbose: Whether to print progress\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with text detection results\n",
                "    \"\"\"\n",
                "    if not TESSERACT_AVAILABLE:\n",
                "        if verbose:\n",
                "            print(\"\ud83d\udcdd Text detection: Tesseract not installed, skipping...\")\n",
                "        return {\n",
                "            \"text_present_ratio\": 0.0,\n",
                "            \"frames_with_text\": 0,\n",
                "            \"total_frames_evaluated\": 0,\n",
                "            \"keywords_top10\": [],\n",
                "            \"available\": False,\n",
                "            \"error\": \"Tesseract OCR not installed\"\n",
                "        }\n",
                "    \n",
                "    # Check if tesseract binary is available\n",
                "    try:\n",
                "        pytesseract.get_tesseract_version()\n",
                "    except pytesseract.TesseractNotFoundError:\n",
                "        if verbose:\n",
                "            print(\"\ud83d\udcdd Text detection: Tesseract binary not in PATH, skipping...\")\n",
                "            print(\"   Please install Tesseract OCR and add it to your system PATH.\")\n",
                "            print(\"   macOS: brew install tesseract\")\n",
                "            print(\"   Windows: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
                "        return {\n",
                "            \"text_present_ratio\": 0.0,\n",
                "            \"frames_with_text\": 0,\n",
                "            \"total_frames_evaluated\": 0,\n",
                "            \"keywords_top10\": [],\n",
                "            \"available\": False,\n",
                "            \"error\": \"Tesseract binary not found in PATH\"\n",
                "        }\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"\ud83d\udcdd Detecting text (OCR)...\")\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(video_path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(video_path))\n",
                "    \n",
                "    try:\n",
                "        frames_evaluated = 0\n",
                "        frames_with_text = 0\n",
                "        keywords: Counter = Counter()\n",
                "        \n",
                "        while True:\n",
                "            ok, frame = capture.read()\n",
                "            if not ok:\n",
                "                break\n",
                "            \n",
                "            frames_evaluated += 1\n",
                "            \n",
                "            # Preprocess frame\n",
                "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "            blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
                "            _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
                "            \n",
                "            # Run OCR\n",
                "            try:\n",
                "                data = pytesseract.image_to_data(\n",
                "                    binary,\n",
                "                    output_type=pytesseract.Output.DICT,\n",
                "                    lang=config.language\n",
                "                )\n",
                "            except Exception:\n",
                "                continue\n",
                "            \n",
                "            # Process results\n",
                "            has_text = False\n",
                "            for word, conf in zip(data.get(\"text\", []), data.get(\"conf\", [])):\n",
                "                if not word or word.isspace():\n",
                "                    continue\n",
                "                try:\n",
                "                    conf_val = float(conf)\n",
                "                except (ValueError, TypeError):\n",
                "                    continue\n",
                "                \n",
                "                if conf_val >= config.min_confidence:\n",
                "                    has_text = True\n",
                "                    cleaned = word.strip().lower()\n",
                "                    if len(cleaned) >= 2 and cleaned.isalnum():\n",
                "                        keywords[cleaned] += 1\n",
                "            \n",
                "            if has_text:\n",
                "                frames_with_text += 1\n",
                "            \n",
                "            # Frame stepping\n",
                "            if config.frame_step > 1:\n",
                "                for _ in range(config.frame_step - 1):\n",
                "                    if not capture.grab():\n",
                "                        break\n",
                "    finally:\n",
                "        capture.release()\n",
                "    \n",
                "    ratio = frames_with_text / frames_evaluated if frames_evaluated > 0 else 0.0\n",
                "    top_keywords = [word for word, _ in keywords.most_common(10)]\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"   \u2713 Text in {frames_with_text}/{frames_evaluated} frames sampled ({ratio:.1%})\")\n",
                "    \n",
                "    return {\n",
                "        \"text_present_ratio\": round(ratio, 4),\n",
                "        \"frames_with_text\": frames_with_text,\n",
                "        \"total_frames_evaluated\": frames_evaluated,\n",
                "        \"keywords_top10\": top_keywords,\n",
                "        \"unique_words\": len(keywords),\n",
                "        \"frame_step_used\": config.frame_step,\n",
                "        \"min_confidence\": config.min_confidence,\n",
                "        \"available\": True,\n",
                "    }\n",
                "\n",
                "print(\"\u2705 Text detection defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Object detection defined\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# OBJECT/PERSON DETECTION (YOLO)\n",
                "# ============================================================================\n",
                "\n",
                "# Global model cache\n",
                "_yolo_model = None\n",
                "\n",
                "def detect_objects(\n",
                "    video_path: Path,\n",
                "    config: ObjectDetectionConfig,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Detect objects and people in video using YOLOv8.\n",
                "    Uses tracking to count unique persons.\n",
                "    \n",
                "    Args:\n",
                "        video_path: Path to the video file\n",
                "        config: Object detection configuration\n",
                "        verbose: Whether to print progress\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with object detection results\n",
                "    \"\"\"\n",
                "    global _yolo_model\n",
                "    \n",
                "    if not YOLO_AVAILABLE:\n",
                "        if verbose:\n",
                "            print(\"\ud83c\udfaf Object detection: YOLO not installed, skipping...\")\n",
                "        return {\n",
                "            \"persons_detected\": 0,\n",
                "            \"unique_persons\": 0,\n",
                "            \"objects_detected\": 0,\n",
                "            \"person_ratio\": 0.0,\n",
                "            \"object_ratio\": 0.0,\n",
                "            \"dominant_category\": \"unknown\",\n",
                "            \"frames_evaluated\": 0,\n",
                "            \"available\": False,\n",
                "            \"error\": \"ultralytics package not installed\"\n",
                "        }\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"\ud83c\udfaf Detecting objects with YOLOv8 (Tracking enabled)...\")\n",
                "    \n",
                "    # Load model (cached)\n",
                "    if _yolo_model is None:\n",
                "        model_name = f\"yolov8{config.model_size}.pt\"\n",
                "        if verbose:\n",
                "            print(f\"   Loading model: {model_name}\")\n",
                "        _yolo_model = YOLO(model_name)\n",
                "    \n",
                "    model = _yolo_model\n",
                "    \n",
                "    capture = cv2.VideoCapture(str(video_path))\n",
                "    if not capture.isOpened():\n",
                "        raise VideoOpenError(str(video_path))\n",
                "    \n",
                "    try:\n",
                "        persons_detections = 0\n",
                "        objects_detections = 0\n",
                "        unique_person_ids = set()\n",
                "        class_counts: Dict[str, int] = {}\n",
                "        frames_evaluated = 0\n",
                "        \n",
                "        while True:\n",
                "            ok, frame = capture.read()\n",
                "            if not ok:\n",
                "                break\n",
                "            \n",
                "            frames_evaluated += 1\n",
                "            \n",
                "            # Run YOLO inference with tracking\n",
                "            # persist=True is important for tracking objects between frames\n",
                "            results = model.track(\n",
                "                frame,\n",
                "                persist=True,\n",
                "                conf=config.confidence_threshold,\n",
                "                verbose=False,\n",
                "                device=\"cuda\" if config.use_gpu else \"cpu\"\n",
                "            )\n",
                "            \n",
                "            # Process detections\n",
                "            for result in results:\n",
                "                boxes = result.boxes\n",
                "                if boxes is None:\n",
                "                    continue\n",
                "                \n",
                "                # Iterate through detected boxes\n",
                "                for i, box in enumerate(boxes):\n",
                "                    class_id = int(box.cls[0])\n",
                "                    class_name = model.names[class_id]\n",
                "                    \n",
                "                    class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
                "                    \n",
                "                    if class_id == 0:  # Person class\n",
                "                        persons_detections += 1\n",
                "                        # Get tracking ID if available\n",
                "                        if box.id is not None:\n",
                "                            track_id = int(box.id[0])\n",
                "                            unique_person_ids.add(track_id)\n",
                "                    else:\n",
                "                        objects_detections += 1\n",
                "            \n",
                "            # Frame stepping\n",
                "            if config.frame_step > 1:\n",
                "                for _ in range(config.frame_step - 1):\n",
                "                    if not capture.grab():\n",
                "                        break\n",
                "    finally:\n",
                "        capture.release()\n",
                "    \n",
                "    # Calculate ratios based on total detections for presence\n",
                "    total = persons_detections + objects_detections\n",
                "    person_ratio = persons_detections / total if total > 0 else 0.0\n",
                "    object_ratio = objects_detections / total if total > 0 else 0.0\n",
                "    \n",
                "    if persons_detections > objects_detections:\n",
                "        dominant = \"person\"\n",
                "    elif objects_detections > persons_detections:\n",
                "        dominant = \"object\"\n",
                "    else:\n",
                "        dominant = \"tie\"\n",
                "    \n",
                "    sorted_classes = dict(sorted(class_counts.items(), key=lambda x: x[1], reverse=True))\n",
                "    unique_persons_count = len(unique_person_ids)\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"   \u2713 Total Detections: {persons_detections} persons, {objects_detections} objects\")\n",
                "        print(f\"   \u2713 Unique Persons Tracked: {unique_persons_count}\")\n",
                "    \n",
                "    return {\n",
                "        \"persons_detected\": persons_detections,\n",
                "        \"unique_persons\": unique_persons_count,\n",
                "        \"objects_detected\": objects_detections,\n",
                "        \"person_ratio\": round(person_ratio, 4),\n",
                "        \"object_ratio\": round(object_ratio, 4),\n",
                "        \"dominant_category\": dominant,\n",
                "        \"class_distribution\": sorted_classes,\n",
                "        \"top_classes\": list(sorted_classes.keys())[:10],\n",
                "        \"frames_evaluated\": frames_evaluated,\n",
                "        \"frame_step_used\": config.frame_step,\n",
                "        \"confidence_threshold\": config.confidence_threshold,\n",
                "        \"model_used\": f\"yolov8{config.model_size}\",\n",
                "        \"available\": True,\n",
                "    }\n",
                "\n",
                "print(\"\u2705 Object detection defined (with tracking)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 VideoFeatureExtractor class defined\n",
                        "\n",
                        "\ud83c\udf89 All components loaded! Ready to extract features.\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# MAIN EXTRACTOR CLASS\n",
                "# ============================================================================\n",
                "\n",
                "class VideoFeatureExtractor:\n",
                "    \"\"\"\n",
                "    Main class for extracting features from video files.\n",
                "    \n",
                "    Example:\n",
                "        extractor = VideoFeatureExtractor()\n",
                "        results = extractor.extract(\"video.mp4\", features=[\"cuts\", \"motion\"])\n",
                "    \"\"\"\n",
                "    \n",
                "    VERSION = \"2.0.0\"\n",
                "    AVAILABLE_FEATURES = {\"cuts\", \"motion\", \"text\", \"objects\"}\n",
                "    \n",
                "    def __init__(self, config: ExtractorConfig = None):\n",
                "        self.config = config or ExtractorConfig()\n",
                "    \n",
                "    def check_availability(self) -> Dict[str, bool]:\n",
                "        \"\"\"Check which features are available.\"\"\"\n",
                "        return {\n",
                "            \"cuts\": True,  # Always available (OpenCV)\n",
                "            \"motion\": True,  # Always available (OpenCV)\n",
                "            \"text\": TESSERACT_AVAILABLE and self._check_tesseract(),\n",
                "            \"objects\": YOLO_AVAILABLE,\n",
                "        }\n",
                "    \n",
                "    def _check_tesseract(self) -> bool:\n",
                "        \"\"\"Check if Tesseract binary is available.\"\"\"\n",
                "        if not TESSERACT_AVAILABLE:\n",
                "            return False\n",
                "        try:\n",
                "            pytesseract.get_tesseract_version()\n",
                "            return True\n",
                "        except:\n",
                "            return False\n",
                "    \n",
                "    def extract(\n",
                "        self,\n",
                "        video_path: str | Path,\n",
                "        features: List[str] = None,\n",
                "        include_metadata: bool = True,\n",
                "        verbose: bool = True\n",
                "    ) -> Dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Extract features from a video file.\n",
                "        \n",
                "        Args:\n",
                "            video_path: Path to the video file\n",
                "            features: List of features to extract (default: all available)\n",
                "            include_metadata: Whether to include video metadata\n",
                "            verbose: Whether to print progress\n",
                "            \n",
                "        Returns:\n",
                "            Dictionary with all extraction results\n",
                "        \"\"\"\n",
                "        start_time = datetime.now()\n",
                "        video_path = Path(video_path)\n",
                "        \n",
                "        # Validate video\n",
                "        video_path = validate_video_file(video_path)\n",
                "        \n",
                "        # Determine features\n",
                "        if features is None:\n",
                "            features = list(self.AVAILABLE_FEATURES)\n",
                "        \n",
                "        # Validate features\n",
                "        invalid = set(features) - self.AVAILABLE_FEATURES\n",
                "        if invalid:\n",
                "            raise InvalidFeatureError(features, self.AVAILABLE_FEATURES)\n",
                "        \n",
                "        if verbose:\n",
                "            print(f\"\\n{'='*60}\")\n",
                "            print(f\"\ud83c\udfac Video Feature Extraction v{self.VERSION}\")\n",
                "            print(f\"{'='*60}\")\n",
                "            print(f\"\ud83d\udcc1 Video: {video_path.name}\")\n",
                "            print(f\"\ud83d\udccb Features: {', '.join(features)}\")\n",
                "            print(f\"{'='*60}\\n\")\n",
                "        \n",
                "        # Initialize output\n",
                "        output: Dict[str, Any] = {\n",
                "            \"video_path\": str(video_path),\n",
                "            \"extraction_timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
                "            \"features_requested\": features,\n",
                "            \"extractor_version\": self.VERSION,\n",
                "        }\n",
                "        \n",
                "        # Add metadata\n",
                "        if include_metadata:\n",
                "            try:\n",
                "                metadata = get_video_metadata(video_path)\n",
                "                output[\"video_metadata\"] = metadata.to_dict()\n",
                "            except Exception as e:\n",
                "                output[\"video_metadata\"] = {\"error\": str(e)}\n",
                "        \n",
                "        # Extract features\n",
                "        results: Dict[str, Any] = {}\n",
                "        \n",
                "        if \"cuts\" in features:\n",
                "            results[\"shot_cut_detection\"] = detect_shot_cuts(\n",
                "                video_path, self.config.shot_cut, verbose\n",
                "            )\n",
                "        \n",
                "        if \"motion\" in features:\n",
                "            results[\"motion_analysis\"] = analyze_motion(\n",
                "                video_path, self.config.motion, verbose\n",
                "            )\n",
                "        \n",
                "        if \"text\" in features:\n",
                "            results[\"text_detection\"] = detect_text(\n",
                "                video_path, self.config.text_detection, verbose\n",
                "            )\n",
                "        \n",
                "        if \"objects\" in features:\n",
                "            results[\"object_person_dominance\"] = detect_objects(\n",
                "                video_path, self.config.object_detection, verbose\n",
                "            )\n",
                "        \n",
                "        output[\"results\"] = results\n",
                "        \n",
                "        # Calculate processing time\n",
                "        elapsed = (datetime.now() - start_time).total_seconds()\n",
                "        output[\"processing_time_seconds\"] = round(elapsed, 2)\n",
                "        \n",
                "        if verbose:\n",
                "            print(f\"\\n{'='*60}\")\n",
                "            print(f\"\u2705 Extraction complete in {elapsed:.2f} seconds\")\n",
                "            print(f\"{'='*60}\")\n",
                "        \n",
                "        return output\n",
                "    \n",
                "    def extract_to_json(\n",
                "        self,\n",
                "        video_path: str | Path,\n",
                "        output_path: str | Path = None,\n",
                "        features: List[str] = None,\n",
                "        pretty: bool = True,\n",
                "        verbose: bool = True\n",
                "    ) -> str:\n",
                "        \"\"\"Extract features and return/save as JSON.\"\"\"\n",
                "        results = self.extract(video_path, features, verbose=verbose)\n",
                "        \n",
                "        indent = 2 if pretty else None\n",
                "        json_output = json.dumps(results, indent=indent, default=str)\n",
                "        \n",
                "        if output_path:\n",
                "            Path(output_path).write_text(json_output)\n",
                "            if verbose:\n",
                "                print(f\"\ud83d\udcbe Saved to: {output_path}\")\n",
                "        \n",
                "        return json_output\n",
                "\n",
                "print(\"\u2705 VideoFeatureExtractor class defined\")\n",
                "print(f\"\\n\ud83c\udf89 All components loaded! Ready to extract features.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Usage Examples\n",
                "\n",
                "Now let's use the extractor to analyze videos!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\ud83d\udccb Feature Availability:\n",
                        "----------------------------------------\n",
                        "   cuts       \u2705 Available\n",
                        "   motion     \u2705 Available\n",
                        "   text       \u274c Not available\n",
                        "   objects    \u2705 Available\n"
                    ]
                }
            ],
            "source": [
                "# Create extractor instance\n",
                "extractor = VideoFeatureExtractor()\n",
                "\n",
                "# Check feature availability\n",
                "print(\"\ud83d\udccb Feature Availability:\")\n",
                "print(\"-\" * 40)\n",
                "for feature, available in extractor.check_availability().items():\n",
                "    status = \"\u2705 Available\" if available else \"\u274c Not available\"\n",
                "    print(f\"   {feature:10} {status}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Video found: videoplayback.mp4\n",
                        "\n",
                        "\ud83d\udcf9 Video Info:\n",
                        "   Resolution: 360x640\n",
                        "   Duration:   11.70s\n",
                        "   FPS:        30.0\n",
                        "   Frames:     351\n"
                    ]
                }
            ],
            "source": [
                "# Set video path - UPDATE THIS TO YOUR VIDEO\n",
                "VIDEO_PATH = Path(\"videoplayback.mp4\")\n",
                "\n",
                "if VIDEO_PATH.exists():\n",
                "    print(f\"\u2705 Video found: {VIDEO_PATH}\")\n",
                "    \n",
                "    # Get metadata\n",
                "    metadata = get_video_metadata(VIDEO_PATH)\n",
                "    print(f\"\\n\ud83d\udcf9 Video Info:\")\n",
                "    print(f\"   Resolution: {metadata.width}x{metadata.height}\")\n",
                "    print(f\"   Duration:   {metadata.duration_seconds:.2f}s\")\n",
                "    print(f\"   FPS:        {metadata.fps}\")\n",
                "    print(f\"   Frames:     {metadata.total_frames}\")\n",
                "else:\n",
                "    print(f\"\u274c Video not found: {VIDEO_PATH}\")\n",
                "    print(\"   Please update VIDEO_PATH to point to your video file.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Extract Shot Cuts and Motion (Fast)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "\ud83c\udfac Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "\ud83d\udcc1 Video: videoplayback.mp4\n",
                        "\ud83d\udccb Features: cuts, motion\n",
                        "============================================================\n",
                        "\n",
                        "\ud83c\udfac Detecting shot cuts...\n",
                        "   \u2713 Found 0 cuts\n",
                        "\ud83c\udfc3 Analyzing motion...\n",
                        "   \u2713 Avg motion: 0.9316\n",
                        "\n",
                        "============================================================\n",
                        "\u2705 Extraction complete in 2.64 seconds\n",
                        "============================================================\n",
                        "\n",
                        "\ud83d\udcca RESULTS:\n",
                        "----------------------------------------\n",
                        "\n",
                        "\ud83c\udfac Shot Cuts:\n",
                        "   Count: 0\n",
                        "\n",
                        "\ud83c\udfc3 Motion:\n",
                        "   Average: 0.9316\n",
                        "   Max:     2.8623\n",
                        "   Std Dev: 0.5769\n"
                    ]
                }
            ],
            "source": [
                "# Extract fast features\n",
                "if VIDEO_PATH.exists():\n",
                "    results = extractor.extract(\n",
                "        VIDEO_PATH,\n",
                "        features=[\"cuts\", \"motion\"]\n",
                "    )\n",
                "    \n",
                "    # Display results\n",
                "    print(\"\\n\ud83d\udcca RESULTS:\")\n",
                "    print(\"-\" * 40)\n",
                "    \n",
                "    cuts = results[\"results\"][\"shot_cut_detection\"]\n",
                "    print(f\"\\n\ud83c\udfac Shot Cuts:\")\n",
                "    print(f\"   Count: {cuts['shot_cut_count']}\")\n",
                "    \n",
                "    motion = results[\"results\"][\"motion_analysis\"]\n",
                "    print(f\"\\n\ud83c\udfc3 Motion:\")\n",
                "    print(f\"   Average: {motion['average_motion_magnitude']:.4f}\")\n",
                "    print(f\"   Max:     {motion['max_motion_magnitude']:.4f}\")\n",
                "    print(f\"   Std Dev: {motion['motion_std']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Object Detection with YOLOv8"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "\ud83c\udfac Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "\ud83d\udcc1 Video: videoplayback.mp4\n",
                        "\ud83d\udccb Features: objects\n",
                        "============================================================\n",
                        "\n",
                        "\ud83c\udfaf Detecting objects with YOLOv8...\n",
                        "   Loading model: yolov8n.pt\n",
                        "   \u2713 24 persons, 0 objects detected\n",
                        "\n",
                        "============================================================\n",
                        "\u2705 Extraction complete in 0.57 seconds\n",
                        "============================================================\n",
                        "\n",
                        "\ud83c\udfaf Object Detection Results:\n",
                        "----------------------------------------\n",
                        "   Persons:  24\n",
                        "   Objects:  0\n",
                        "   Dominant: PERSON\n",
                        "\n",
                        "   \ud83d\udce6 Top Classes:\n",
                        "      person: 24\n"
                    ]
                }
            ],
            "source": [
                "# Extract objects (may download model on first run)\n",
                "if VIDEO_PATH.exists():\n",
                "    object_results = extractor.extract(\n",
                "        VIDEO_PATH,\n",
                "        features=[\"objects\"]\n",
                "    )\n",
                "    \n",
                "    obj = object_results[\"results\"][\"object_person_dominance\"]\n",
                "    \n",
                "    if obj.get(\"available\", True):\n",
                "        print(\"\\n\ud83c\udfaf Object Detection Results:\")\n",
                "        print(\"-\" * 40)\n",
                "        print(f\"   Persons:  {obj['persons_detected']}\")\n",
                "        print(f\"   Objects:  {obj['objects_detected']}\")\n",
                "        print(f\"   Dominant: {obj['dominant_category'].upper()}\")\n",
                "        \n",
                "        if obj.get('class_distribution'):\n",
                "            print(\"\\n   \ud83d\udce6 Top Classes:\")\n",
                "            for cls, count in list(obj['class_distribution'].items())[:5]:\n",
                "                print(f\"      {cls}: {count}\")\n",
                "    else:\n",
                "        print(f\"   \u26a0\ufe0f {obj.get('error', 'Not available')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Custom Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 Custom configuration created\n",
                        "   Shot cut threshold: 20.0\n",
                        "   Motion frame step:  5\n",
                        "   Object confidence:  0.7\n"
                    ]
                }
            ],
            "source": [
                "# Create custom configuration\n",
                "custom_config = ExtractorConfig()\n",
                "\n",
                "# More sensitive shot cut detection\n",
                "custom_config.shot_cut.diff_threshold = 20.0  # Lower = more sensitive\n",
                "\n",
                "# Faster motion analysis\n",
                "custom_config.motion.frame_step = 5  # Skip more frames\n",
                "\n",
                "# Higher confidence for objects\n",
                "custom_config.object_detection.confidence_threshold = 0.7\n",
                "\n",
                "# Create new extractor with custom config\n",
                "custom_extractor = VideoFeatureExtractor(custom_config)\n",
                "\n",
                "print(\"\u2705 Custom configuration created\")\n",
                "print(f\"   Shot cut threshold: {custom_config.shot_cut.diff_threshold}\")\n",
                "print(f\"   Motion frame step:  {custom_config.motion.frame_step}\")\n",
                "print(f\"   Object confidence:  {custom_config.object_detection.confidence_threshold}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "\ud83c\udfac Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "\ud83d\udcc1 Video: videoplayback.mp4\n",
                        "\ud83d\udccb Features: cuts, motion\n",
                        "============================================================\n",
                        "\n",
                        "\ud83c\udfac Detecting shot cuts...\n",
                        "   \u2713 Found 0 cuts\n",
                        "\ud83c\udfc3 Analyzing motion...\n",
                        "   \u2713 Avg motion: 1.9809\n",
                        "\n",
                        "============================================================\n",
                        "\u2705 Extraction complete in 1.09 seconds\n",
                        "============================================================\n",
                        "\n",
                        "\ud83d\udcca Custom Config Results:\n",
                        "   Cuts found: 0\n",
                        "   Motion avg: 1.9809\n"
                    ]
                }
            ],
            "source": [
                "# Run with custom config\n",
                "if VIDEO_PATH.exists():\n",
                "    custom_results = custom_extractor.extract(\n",
                "        VIDEO_PATH,\n",
                "        features=[\"cuts\", \"motion\"]\n",
                "    )\n",
                "    \n",
                "    print(\"\\n\ud83d\udcca Custom Config Results:\")\n",
                "    print(f\"   Cuts found: {custom_results['results']['shot_cut_detection']['shot_cut_count']}\")\n",
                "    print(f\"   Motion avg: {custom_results['results']['motion_analysis']['average_motion_magnitude']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Export to JSON"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "\ud83c\udfac Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "\ud83d\udcc1 Video: videoplayback.mp4\n",
                        "\ud83d\udccb Features: cuts, motion\n",
                        "============================================================\n",
                        "\n",
                        "\ud83c\udfac Detecting shot cuts...\n",
                        "   \u2713 Found 0 cuts\n",
                        "\ud83c\udfc3 Analyzing motion...\n",
                        "   \u2713 Avg motion: 0.9316\n",
                        "\n",
                        "============================================================\n",
                        "\u2705 Extraction complete in 2.55 seconds\n",
                        "============================================================\n",
                        "\ud83d\udcbe Saved to: video_analysis_results.json\n",
                        "\n",
                        "\ud83d\udcc4 JSON Preview (first 500 chars):\n",
                        "{\n",
                        "  \"video_path\": \"/Users/ridam/Desktop/White Panda/videoplayback.mp4\",\n",
                        "  \"extraction_timestamp\": \"2026-01-14T09:38:57.626928Z\",\n",
                        "  \"features_requested\": [\n",
                        "    \"cuts\",\n",
                        "    \"motion\"\n",
                        "  ],\n",
                        "  \"extractor_version\": \"2.0.0\",\n",
                        "  \"video_metadata\": {\n",
                        "    \"path\": \"/Users/ridam/Desktop/White Panda/videoplayback.mp4\",\n",
                        "    \"resolution\": {\n",
                        "      \"width\": 360,\n",
                        "      \"height\": 640\n",
                        "    },\n",
                        "    \"fps\": 30.0,\n",
                        "    \"total_frames\": 351,\n",
                        "    \"duration_seconds\": 11.7,\n",
                        "    \"codec\": \"h264\",\n",
                        "    \"file_size_bytes\": 877848\n",
                        "  },\n",
                        "...\n"
                    ]
                }
            ],
            "source": [
                "# Export full results to JSON\n",
                "if VIDEO_PATH.exists():\n",
                "    output_file = \"video_analysis_results.json\"\n",
                "    \n",
                "    json_output = extractor.extract_to_json(\n",
                "        VIDEO_PATH,\n",
                "        output_path=output_file,\n",
                "        features=[\"cuts\", \"motion\"],\n",
                "        pretty=True\n",
                "    )\n",
                "    \n",
                "    print(f\"\\n\ud83d\udcc4 JSON Preview (first 500 chars):\")\n",
                "    print(json_output[:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5 Full Extraction (All Features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running features: ['cuts', 'motion', 'objects']\n",
                        "\n",
                        "============================================================\n",
                        "\ud83c\udfac Video Feature Extraction v2.0.0\n",
                        "============================================================\n",
                        "\ud83d\udcc1 Video: videoplayback.mp4\n",
                        "\ud83d\udccb Features: cuts, motion, objects\n",
                        "============================================================\n",
                        "\n",
                        "\ud83c\udfac Detecting shot cuts...\n",
                        "   \u2713 Found 0 cuts\n",
                        "\ud83c\udfc3 Analyzing motion...\n",
                        "   \u2713 Avg motion: 0.9316\n",
                        "\ud83c\udfaf Detecting objects with YOLOv8...\n",
                        "   \u2713 24 persons, 0 objects detected\n",
                        "\n",
                        "============================================================\n",
                        "\u2705 Extraction complete in 3.01 seconds\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "# Run all available features\n",
                "if VIDEO_PATH.exists():\n",
                "    availability = extractor.check_availability()\n",
                "    available_features = [f for f, avail in availability.items() if avail]\n",
                "    \n",
                "    print(f\"Running features: {available_features}\")\n",
                "    \n",
                "    full_results = extractor.extract(\n",
                "        VIDEO_PATH,\n",
                "        features=available_features\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "\ud83d\udcca COMPLETE EXTRACTION SUMMARY\n",
                        "============================================================\n",
                        "\n",
                        "\ud83d\udcc1 Video: videoplayback.mp4\n",
                        "\u23f1\ufe0f  Time:  3.01s\n",
                        "\n",
                        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
                        "\ud83d\udccb Shot Cut Detection:\n",
                        "   shot_cut_count: 0\n",
                        "   cut_frames: 0 items\n",
                        "   frame_step_used: 1\n",
                        "   mean_diff_threshold: 30.0000\n",
                        "   min_gap_frames: 5\n",
                        "\n",
                        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
                        "\ud83d\udccb Motion Analysis:\n",
                        "   average_motion_magnitude: 0.9316\n",
                        "   max_motion_magnitude: 2.8623\n",
                        "   min_motion_magnitude: 0.2001\n",
                        "   motion_std: 0.5769\n",
                        "   motion_samples: 175\n",
                        "\n",
                        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
                        "\ud83d\udccb Object Person Dominance:\n",
                        "   persons_detected: 24\n",
                        "   objects_detected: 0\n",
                        "   person_ratio: 1.0000\n",
                        "   object_ratio: 0.0000\n",
                        "   dominant_category: person\n"
                    ]
                }
            ],
            "source": [
                "# Display comprehensive summary\n",
                "if VIDEO_PATH.exists():\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"\ud83d\udcca COMPLETE EXTRACTION SUMMARY\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    print(f\"\\n\ud83d\udcc1 Video: {full_results['video_path'].split('/')[-1]}\")\n",
                "    print(f\"\u23f1\ufe0f  Time:  {full_results['processing_time_seconds']}s\")\n",
                "    \n",
                "    for key, data in full_results['results'].items():\n",
                "        print(f\"\\n{'\u2500' * 50}\")\n",
                "        print(f\"\ud83d\udccb {key.replace('_', ' ').title()}:\")\n",
                "        \n",
                "        if 'error' in data:\n",
                "            print(f\"   \u26a0\ufe0f {data['error']}\")\n",
                "        else:\n",
                "            for k, v in list(data.items())[:5]:\n",
                "                if isinstance(v, float):\n",
                "                    print(f\"   {k}: {v:.4f}\")\n",
                "                elif isinstance(v, list):\n",
                "                    print(f\"   {k}: {len(v)} items\")\n",
                "                elif isinstance(v, dict):\n",
                "                    print(f\"   {k}: {len(v)} entries\")\n",
                "                else:\n",
                "                    print(f\"   {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Error Handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2705 VideoNotFoundError caught: Video file not found: /nonexistent/video.mp4\n",
                        "\u2705 InvalidFeatureError caught: Invalid feature(s): {'invalid'}. Valid: {'objects', 'motion', 'cuts', 'text'}\n"
                    ]
                }
            ],
            "source": [
                "# Example: Handle missing video\n",
                "try:\n",
                "    extractor.extract(\"/nonexistent/video.mp4\", verbose=False)\n",
                "except VideoNotFoundError as e:\n",
                "    print(f\"\u2705 VideoNotFoundError caught: {e.message}\")\n",
                "\n",
                "# Example: Handle invalid feature\n",
                "try:\n",
                "    extractor.extract(VIDEO_PATH, features=[\"invalid\"], verbose=False)\n",
                "except InvalidFeatureError as e:\n",
                "    print(f\"\u2705 InvalidFeatureError caught: {e.message}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\ud83d\uddd1\ufe0f Removed: video_analysis_results.json\n",
                        "\n",
                        "\u2705 Cleanup complete!\n"
                    ]
                }
            ],
            "source": [
                "# Clean up temporary files\n",
                "import os\n",
                "\n",
                "temp_files = [\"video_analysis_results.json\"]\n",
                "\n",
                "for f in temp_files:\n",
                "    if Path(f).exists():\n",
                "        os.remove(f)\n",
                "        print(f\"\ud83d\uddd1\ufe0f Removed: {f}\")\n",
                "\n",
                "print(\"\\n\u2705 Cleanup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83d\udcda Quick Reference\n",
                "\n",
                "### Available Features\n",
                "| Feature | Description | Requirements |\n",
                "|---------|-------------|-------------|\n",
                "| `cuts` | Shot cut detection | OpenCV |\n",
                "| `motion` | Motion analysis | OpenCV |\n",
                "| `text` | OCR text detection | pytesseract + Tesseract |\n",
                "| `objects` | Object/person detection | ultralytics (YOLOv8) |\n",
                "\n",
                "### Configuration Options\n",
                "```python\n",
                "config = ExtractorConfig()\n",
                "\n",
                "# Shot cut settings\n",
                "config.shot_cut.diff_threshold = 30.0  # Lower = more sensitive\n",
                "config.shot_cut.min_gap_frames = 5\n",
                "\n",
                "# Motion settings\n",
                "config.motion.frame_step = 2  # Higher = faster\n",
                "\n",
                "# Object detection settings\n",
                "config.object_detection.confidence_threshold = 0.5\n",
                "config.object_detection.model_size = \"n\"  # n, s, m, l, x\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}